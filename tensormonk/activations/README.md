### activations
* Activations: A supporting function for convolution and linear layers. Available options:
  - relu
  - sigm
  - tanh
  - [relu6](http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf)
  - [lklu](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)
  - [elu](https://arxiv.org/pdf/1511.07289.pdf)
  - [gelu](https://arxiv.org/pdf/1606.08415.pdf)
  - [maxo](https://arxiv.org/pdf/1302.4389.pdf)
  - [mish](https://arxiv.org/pdf/1908.08681.pdf)
  - [prelu](https://arxiv.org/pdf/1502.01852.pdf)
  - rmxo
  - [selu](https://arxiv.org/pdf/1706.02515.pdf)
  - [squash](https://arxiv.org/pdf/1710.09829.pdf)
  - [swish](https://arxiv.org/pdf/1710.05941v1.pdf)
