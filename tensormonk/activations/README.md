### Activations
* Activations: A supporting function for convolution and linear layers. Available options:
  - relu
  - sigm
  - tanh
  - relu6  - [Convolutional Deep Belief Networks on CIFAR-10](http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf)
  - lklu   - [Rectifier Nonlinearities Improve Neural Network Acoustic Models](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)
  - elu    - [FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)](https://arxiv.org/pdf/1511.07289.pdf)
  - gelu   - [GAUSSIAN ERROR LINEAR UNITS (GELUS)](https://arxiv.org/pdf/1606.08415.pdf)
  - maxo   - [Maxout Networks](https://arxiv.org/pdf/1302.4389.pdf)
  - mish   - [Mish: A Self Regularized Non-Monotonic Neural Activation Function](https://arxiv.org/pdf/1908.08681.pdf)
  - prelu  - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)
  - rmxo
  - selu   - [Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf)
  - squash - [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf)
  - swish  - [SWISH: A SELF-GATED ACTIVATION FUNCTION](https://arxiv.org/pdf/1710.05941v1.pdf)
